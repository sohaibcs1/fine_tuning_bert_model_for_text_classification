{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning BERT for Spam Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        " \n",
        "#  drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ-4U-5rwN-V",
        "outputId": "270e15e8-635f-47a4-8094-55d80b1cfd12"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = 'gdrive/MyDrive/dataset_interface/FinalDataset.csv'"
      ],
      "metadata": {
        "id": "wRXQW1ygwYyM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(root_path)\n",
        "df=df[[\"Category\",\"Description\"]]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CQnuqZEawZUT",
        "outputId": "7c1c32e4-fe96-4da4-82e4-b929cbbc3d2d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Category                                        Description\n",
              "0           Content   Fantastic program! Instant search results and...\n",
              "1           Content   Great browser nice and fast. Please add force...\n",
              "2        Uniformity   The best and Light-weight browser on Android....\n",
              "3        Uniformity   Thank you dev.  Love the ui.  Love the smooth...\n",
              "4        Navigation   Yeah... I'm greedy. First and foremost - supp...\n",
              "...             ...                                                ...\n",
              "2393        Content  I am only giving this app three star because s...\n",
              "2394        Content  its so helpfull for me having online class u c...\n",
              "2395  Interactivity  Very nice up... It could be best if sending vo...\n",
              "2396   LoadingSpeed  Why when I want to send files, images, videos ...\n",
              "2397        Content  Very good video & Audio Quality... Overall Ver...\n",
              "\n",
              "[2398 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-298f9799-ec5e-43ab-aceb-0448dd7df84b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Content</td>\n",
              "      <td>Fantastic program! Instant search results and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Content</td>\n",
              "      <td>Great browser nice and fast. Please add force...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Uniformity</td>\n",
              "      <td>The best and Light-weight browser on Android....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Uniformity</td>\n",
              "      <td>Thank you dev.  Love the ui.  Love the smooth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Navigation</td>\n",
              "      <td>Yeah... I'm greedy. First and foremost - supp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2393</th>\n",
              "      <td>Content</td>\n",
              "      <td>I am only giving this app three star because s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2394</th>\n",
              "      <td>Content</td>\n",
              "      <td>its so helpfull for me having online class u c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2395</th>\n",
              "      <td>Interactivity</td>\n",
              "      <td>Very nice up... It could be best if sending vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2396</th>\n",
              "      <td>LoadingSpeed</td>\n",
              "      <td>Why when I want to send files, images, videos ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2397</th>\n",
              "      <td>Content</td>\n",
              "      <td>Very good video &amp; Audio Quality... Overall Ver...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2398 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-298f9799-ec5e-43ab-aceb-0448dd7df84b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-298f9799-ec5e-43ab-aceb-0448dd7df84b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-298f9799-ec5e-43ab-aceb-0448dd7df84b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['Category'].astype('category').cat.codes\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "QmcwGVMqwpo1",
        "outputId": "ef336e1b-e38f-4e2f-ae68-654fb0d86d90"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Category                                        Description  label\n",
              "0        Content   Fantastic program! Instant search results and...      2\n",
              "1        Content   Great browser nice and fast. Please add force...      2\n",
              "2     Uniformity   The best and Light-weight browser on Android....      8\n",
              "3     Uniformity   Thank you dev.  Love the ui.  Love the smooth...      8\n",
              "4     Navigation   Yeah... I'm greedy. First and foremost - supp...      5\n",
              "5   LoadingSpeed   Do not be fooled. This is a very useful brows...      4\n",
              "6  Accessibility   Within a minute of trying the free version I ...      0\n",
              "7  Interactivity   I'll need a few days of usage to make an info...      3\n",
              "8        Content  For me this is the best browser hands down! It...      2\n",
              "9    ColorScheme   Needs a few additions to make it as good as t...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71e0f00e-2b10-4358-bfd4-6f22b20fc011\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Description</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Content</td>\n",
              "      <td>Fantastic program! Instant search results and...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Content</td>\n",
              "      <td>Great browser nice and fast. Please add force...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Uniformity</td>\n",
              "      <td>The best and Light-weight browser on Android....</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Uniformity</td>\n",
              "      <td>Thank you dev.  Love the ui.  Love the smooth...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Navigation</td>\n",
              "      <td>Yeah... I'm greedy. First and foremost - supp...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LoadingSpeed</td>\n",
              "      <td>Do not be fooled. This is a very useful brows...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Accessibility</td>\n",
              "      <td>Within a minute of trying the free version I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Interactivity</td>\n",
              "      <td>I'll need a few days of usage to make an info...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Content</td>\n",
              "      <td>For me this is the best browser hands down! It...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ColorScheme</td>\n",
              "      <td>Needs a few additions to make it as good as t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71e0f00e-2b10-4358-bfd4-6f22b20fc011')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71e0f00e-2b10-4358-bfd4-6f22b20fc011 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71e0f00e-2b10-4358-bfd4-6f22b20fc011');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1a682Tkdsrm",
        "outputId": "be7e655b-1f94-40aa-9c9a-cdfdafcfc652"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 8, 5, 4, 0, 3, 1, 6, 7], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd92933-f3ba-4cb2-9d6c-8b3c2f035a6e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2398, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676DPU1BOPdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6949c3-7d24-4a43-c1d0-8970ed4a2be6"
      },
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    0.259383\n",
              "3    0.226022\n",
              "6    0.156797\n",
              "1    0.105088\n",
              "8    0.078816\n",
              "0    0.073394\n",
              "7    0.069224\n",
              "5    0.018349\n",
              "4    0.012927\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Description'], df['label'], \n",
        "                                                                    random_state=2023, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2023, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zOKeOMeO-DT"
      },
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAH73n39PHLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98eb9131-ef5f-44de-8635-b70cdcf2508a"
      },
      "source": [
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c5427bb9-f338-4575-e2a6-6178e8ddf209"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff7417e2a00>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 100"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.8)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.dropout = nn.Dropout(0.8)\n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,9)\n",
        "      self.dropout = nn.Dropout(0.8)\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b38c41-ddd4-4c7e-eed5-4763c3309038"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.51580849 1.05335844 0.42860792 0.49193785 8.47474747 6.01433692\n",
            " 0.70891424 1.60727969 1.41245791]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 500"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507562ea-d476-414a-b925-2ba97e06f429"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.403\n",
            "Validation Loss: 2.243\n",
            "\n",
            " Epoch 2 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.256\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 3 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.223\n",
            "Validation Loss: 2.224\n",
            "\n",
            " Epoch 4 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.215\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 5 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.207\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 6 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.223\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 7 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.201\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 8 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 9 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.215\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 10 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.209\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 11 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.202\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 12 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.199\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 13 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.216\n",
            "Validation Loss: 2.187\n",
            "\n",
            " Epoch 14 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.192\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 15 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.218\n",
            "Validation Loss: 2.206\n",
            "\n",
            " Epoch 16 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.219\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 17 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 18 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.195\n",
            "\n",
            " Epoch 19 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.212\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 20 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.183\n",
            "Validation Loss: 2.213\n",
            "\n",
            " Epoch 21 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.196\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 22 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.238\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 23 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.187\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 24 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.251\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 25 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 26 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.207\n",
            "\n",
            " Epoch 27 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 28 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 29 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.195\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 30 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.190\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 31 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.209\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 32 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.195\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 33 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 34 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.180\n",
            "Validation Loss: 2.194\n",
            "\n",
            " Epoch 35 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.182\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 36 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 37 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 38 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.197\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 39 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.192\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 40 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 41 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.194\n",
            "\n",
            " Epoch 42 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.195\n",
            "\n",
            " Epoch 43 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.214\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 44 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.182\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 45 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.206\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 46 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 47 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 48 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.204\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 49 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 50 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 51 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.174\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 52 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.178\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 53 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 54 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.178\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 55 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 56 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.203\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 57 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 58 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 59 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.178\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 60 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 61 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.193\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 62 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 63 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.215\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 64 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 65 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.176\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 66 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 67 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.179\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 68 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 69 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 70 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 71 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 72 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.192\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 73 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 74 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.166\n",
            "Validation Loss: 2.218\n",
            "\n",
            " Epoch 75 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 76 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 77 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.179\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 78 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 79 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 80 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 81 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.183\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 82 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.213\n",
            "\n",
            " Epoch 83 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 84 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 85 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 86 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.170\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 87 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 88 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.243\n",
            "\n",
            " Epoch 89 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.203\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 90 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.178\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 91 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.191\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 92 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.187\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 93 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 94 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 95 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.182\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 96 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.186\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 97 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.176\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 98 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.183\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 99 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.231\n",
            "\n",
            " Epoch 100 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.203\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 101 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 102 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.207\n",
            "\n",
            " Epoch 103 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.206\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 104 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.192\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 105 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.168\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 106 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 107 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.183\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 108 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.202\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 109 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 110 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.213\n",
            "\n",
            " Epoch 111 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 112 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 113 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.176\n",
            "Validation Loss: 2.227\n",
            "\n",
            " Epoch 114 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 115 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.175\n",
            "\n",
            " Epoch 116 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.178\n",
            "\n",
            " Epoch 117 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 118 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.219\n",
            "\n",
            " Epoch 119 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.197\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 120 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 121 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 122 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 123 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.182\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 124 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 125 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.187\n",
            "Validation Loss: 2.207\n",
            "\n",
            " Epoch 126 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 127 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 128 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 129 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 130 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 131 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.208\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 132 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.178\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 133 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.225\n",
            "\n",
            " Epoch 134 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.207\n",
            "\n",
            " Epoch 135 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 136 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.215\n",
            "\n",
            " Epoch 137 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.174\n",
            "Validation Loss: 2.225\n",
            "\n",
            " Epoch 138 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.223\n",
            "\n",
            " Epoch 139 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 140 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.212\n",
            "\n",
            " Epoch 141 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.211\n",
            "\n",
            " Epoch 142 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.198\n",
            "Validation Loss: 2.233\n",
            "\n",
            " Epoch 143 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.165\n",
            "Validation Loss: 2.212\n",
            "\n",
            " Epoch 144 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 145 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 146 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 147 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.194\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 148 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 149 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 150 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.193\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 151 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 152 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 153 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 154 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 155 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.187\n",
            "Validation Loss: 2.172\n",
            "\n",
            " Epoch 156 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.180\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 157 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 158 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.174\n",
            "Validation Loss: 2.224\n",
            "\n",
            " Epoch 159 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 160 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 161 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 162 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.170\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 163 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 164 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 165 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 166 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.220\n",
            "\n",
            " Epoch 167 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 168 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 169 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 170 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.133\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 171 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 172 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.143\n",
            "Validation Loss: 2.204\n",
            "\n",
            " Epoch 173 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.213\n",
            "\n",
            " Epoch 174 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.254\n",
            "\n",
            " Epoch 175 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 176 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.278\n",
            "\n",
            " Epoch 177 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 178 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.176\n",
            "Validation Loss: 2.264\n",
            "\n",
            " Epoch 179 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 180 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 181 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.192\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 182 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 183 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.200\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 184 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.173\n",
            "Validation Loss: 2.206\n",
            "\n",
            " Epoch 185 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.220\n",
            "\n",
            " Epoch 186 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.235\n",
            "\n",
            " Epoch 187 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.208\n",
            "Validation Loss: 2.187\n",
            "\n",
            " Epoch 188 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.207\n",
            "\n",
            " Epoch 189 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 190 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.220\n",
            "Validation Loss: 2.211\n",
            "\n",
            " Epoch 191 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.188\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 192 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 193 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 194 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 195 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.218\n",
            "\n",
            " Epoch 196 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.246\n",
            "\n",
            " Epoch 197 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.232\n",
            "\n",
            " Epoch 198 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.205\n",
            "\n",
            " Epoch 199 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 200 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.172\n",
            "\n",
            " Epoch 201 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.209\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 202 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 203 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.186\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 204 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 205 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 206 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.206\n",
            "\n",
            " Epoch 207 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 208 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 209 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 210 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.213\n",
            "Validation Loss: 2.173\n",
            "\n",
            " Epoch 211 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 212 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 213 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.184\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 214 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.170\n",
            "\n",
            " Epoch 215 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 216 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.172\n",
            "\n",
            " Epoch 217 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.176\n",
            "\n",
            " Epoch 218 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.211\n",
            "\n",
            " Epoch 219 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 220 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.175\n",
            "\n",
            " Epoch 221 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 222 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.194\n",
            "\n",
            " Epoch 223 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 224 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.174\n",
            "Validation Loss: 2.222\n",
            "\n",
            " Epoch 225 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 226 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.138\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 227 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 228 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.168\n",
            "Validation Loss: 2.171\n",
            "\n",
            " Epoch 229 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.195\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 230 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 231 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 232 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 233 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.169\n",
            "\n",
            " Epoch 234 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.203\n",
            "\n",
            " Epoch 235 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 236 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.157\n",
            "\n",
            " Epoch 237 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.196\n",
            "Validation Loss: 2.155\n",
            "\n",
            " Epoch 238 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.165\n",
            "\n",
            " Epoch 239 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.176\n",
            "\n",
            " Epoch 240 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 241 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.170\n",
            "Validation Loss: 2.165\n",
            "\n",
            " Epoch 242 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.139\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 243 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.169\n",
            "\n",
            " Epoch 244 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 245 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.151\n",
            "\n",
            " Epoch 246 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.135\n",
            "Validation Loss: 2.161\n",
            "\n",
            " Epoch 247 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 248 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 249 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.127\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 250 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 251 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 252 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 253 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.156\n",
            "\n",
            " Epoch 254 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 255 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 256 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.167\n",
            "\n",
            " Epoch 257 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 258 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.220\n",
            "\n",
            " Epoch 259 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.195\n",
            "\n",
            " Epoch 260 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.246\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 261 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.161\n",
            "\n",
            " Epoch 262 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.165\n",
            "Validation Loss: 2.161\n",
            "\n",
            " Epoch 263 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.172\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 264 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.164\n",
            "\n",
            " Epoch 265 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.204\n",
            "Validation Loss: 2.166\n",
            "\n",
            " Epoch 266 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.164\n",
            "\n",
            " Epoch 267 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 268 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.141\n",
            "\n",
            " Epoch 269 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.138\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 270 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 271 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 272 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 273 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.175\n",
            "\n",
            " Epoch 274 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 275 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 276 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.143\n",
            "Validation Loss: 2.148\n",
            "\n",
            " Epoch 277 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.164\n",
            "\n",
            " Epoch 278 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 279 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.170\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 280 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.166\n",
            "Validation Loss: 2.169\n",
            "\n",
            " Epoch 281 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 282 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.175\n",
            "\n",
            " Epoch 283 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.194\n",
            "\n",
            " Epoch 284 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 285 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.158\n",
            "\n",
            " Epoch 286 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.117\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 287 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.170\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 288 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 289 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.127\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 290 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 291 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 292 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 293 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.186\n",
            "Validation Loss: 2.209\n",
            "\n",
            " Epoch 294 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.174\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 295 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.136\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 296 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.152\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 297 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.180\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 298 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.167\n",
            "\n",
            " Epoch 299 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.178\n",
            "\n",
            " Epoch 300 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 301 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 302 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.157\n",
            "\n",
            " Epoch 303 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 304 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 305 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.147\n",
            "Validation Loss: 2.156\n",
            "\n",
            " Epoch 306 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 307 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.200\n",
            "\n",
            " Epoch 308 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 309 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.123\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 310 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.176\n",
            "\n",
            " Epoch 311 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.144\n",
            "Validation Loss: 2.178\n",
            "\n",
            " Epoch 312 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.171\n",
            "\n",
            " Epoch 313 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 314 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.164\n",
            "\n",
            " Epoch 315 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 316 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.185\n",
            "\n",
            " Epoch 317 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.197\n",
            "Validation Loss: 2.171\n",
            "\n",
            " Epoch 318 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.155\n",
            "\n",
            " Epoch 319 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.167\n",
            "\n",
            " Epoch 320 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.134\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 321 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 322 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.165\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 323 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.125\n",
            "Validation Loss: 2.198\n",
            "\n",
            " Epoch 324 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 325 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.147\n",
            "\n",
            " Epoch 326 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.154\n",
            "Validation Loss: 2.155\n",
            "\n",
            " Epoch 327 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 328 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.124\n",
            "Validation Loss: 2.165\n",
            "\n",
            " Epoch 329 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.113\n",
            "Validation Loss: 2.195\n",
            "\n",
            " Epoch 330 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.137\n",
            "Validation Loss: 2.196\n",
            "\n",
            " Epoch 331 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.138\n",
            "Validation Loss: 2.226\n",
            "\n",
            " Epoch 332 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.161\n",
            "\n",
            " Epoch 333 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.137\n",
            "Validation Loss: 2.202\n",
            "\n",
            " Epoch 334 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.136\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 335 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.118\n",
            "Validation Loss: 2.248\n",
            "\n",
            " Epoch 336 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 337 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.119\n",
            "Validation Loss: 2.231\n",
            "\n",
            " Epoch 338 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 339 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.164\n",
            "\n",
            " Epoch 340 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.139\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 341 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.211\n",
            "\n",
            " Epoch 342 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.209\n",
            "\n",
            " Epoch 343 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.133\n",
            "Validation Loss: 2.224\n",
            "\n",
            " Epoch 344 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 345 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 346 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.213\n",
            "\n",
            " Epoch 347 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.239\n",
            "\n",
            " Epoch 348 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.160\n",
            "Validation Loss: 2.231\n",
            "\n",
            " Epoch 349 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.216\n",
            "\n",
            " Epoch 350 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.136\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 351 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.157\n",
            "\n",
            " Epoch 352 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.143\n",
            "Validation Loss: 2.170\n",
            "\n",
            " Epoch 353 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.163\n",
            "\n",
            " Epoch 354 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.142\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 355 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.172\n",
            "\n",
            " Epoch 356 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.218\n",
            "\n",
            " Epoch 357 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.177\n",
            "Validation Loss: 2.285\n",
            "\n",
            " Epoch 358 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.190\n",
            "Validation Loss: 2.183\n",
            "\n",
            " Epoch 359 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.211\n",
            "\n",
            " Epoch 360 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.130\n",
            "Validation Loss: 2.192\n",
            "\n",
            " Epoch 361 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.161\n",
            "Validation Loss: 2.178\n",
            "\n",
            " Epoch 362 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.163\n",
            "\n",
            " Epoch 363 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.144\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 364 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 365 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.175\n",
            "Validation Loss: 2.154\n",
            "\n",
            " Epoch 366 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.197\n",
            "\n",
            " Epoch 367 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 368 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.163\n",
            "Validation Loss: 2.148\n",
            "\n",
            " Epoch 369 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.130\n",
            "Validation Loss: 2.194\n",
            "\n",
            " Epoch 370 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.122\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 371 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.118\n",
            "Validation Loss: 2.153\n",
            "\n",
            " Epoch 372 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 373 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.147\n",
            "\n",
            " Epoch 374 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.146\n",
            "\n",
            " Epoch 375 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.134\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 376 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.135\n",
            "Validation Loss: 2.148\n",
            "\n",
            " Epoch 377 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 378 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.142\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 379 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.159\n",
            "\n",
            " Epoch 380 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 381 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.146\n",
            "Validation Loss: 2.170\n",
            "\n",
            " Epoch 382 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.150\n",
            "\n",
            " Epoch 383 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.119\n",
            "Validation Loss: 2.173\n",
            "\n",
            " Epoch 384 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.142\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 385 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.166\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 386 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.133\n",
            "Validation Loss: 2.171\n",
            "\n",
            " Epoch 387 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.123\n",
            "Validation Loss: 2.176\n",
            "\n",
            " Epoch 388 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.191\n",
            "\n",
            " Epoch 389 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.165\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 390 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.164\n",
            "Validation Loss: 2.210\n",
            "\n",
            " Epoch 391 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.130\n",
            "Validation Loss: 2.189\n",
            "\n",
            " Epoch 392 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.163\n",
            "\n",
            " Epoch 393 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.150\n",
            "Validation Loss: 2.190\n",
            "\n",
            " Epoch 394 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 395 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.240\n",
            "Validation Loss: 2.163\n",
            "\n",
            " Epoch 396 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 397 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.157\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 398 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.138\n",
            "Validation Loss: 2.150\n",
            "\n",
            " Epoch 399 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.135\n",
            "Validation Loss: 2.229\n",
            "\n",
            " Epoch 400 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 401 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.139\n",
            "Validation Loss: 2.208\n",
            "\n",
            " Epoch 402 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.143\n",
            "Validation Loss: 2.167\n",
            "\n",
            " Epoch 403 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.107\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 404 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 405 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.127\n",
            "Validation Loss: 2.176\n",
            "\n",
            " Epoch 406 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.179\n",
            "\n",
            " Epoch 407 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.156\n",
            "Validation Loss: 2.180\n",
            "\n",
            " Epoch 408 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.156\n",
            "\n",
            " Epoch 409 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.168\n",
            "Validation Loss: 2.154\n",
            "\n",
            " Epoch 410 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.111\n",
            "Validation Loss: 2.170\n",
            "\n",
            " Epoch 411 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.168\n",
            "Validation Loss: 2.175\n",
            "\n",
            " Epoch 412 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.108\n",
            "Validation Loss: 2.157\n",
            "\n",
            " Epoch 413 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.139\n",
            "Validation Loss: 2.169\n",
            "\n",
            " Epoch 414 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.149\n",
            "Validation Loss: 2.163\n",
            "\n",
            " Epoch 415 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.149\n",
            "Validation Loss: 2.223\n",
            "\n",
            " Epoch 416 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.201\n",
            "\n",
            " Epoch 417 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.187\n",
            "\n",
            " Epoch 418 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.124\n",
            "Validation Loss: 2.246\n",
            "\n",
            " Epoch 419 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.185\n",
            "Validation Loss: 2.193\n",
            "\n",
            " Epoch 420 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.121\n",
            "Validation Loss: 2.214\n",
            "\n",
            " Epoch 421 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.144\n",
            "Validation Loss: 2.172\n",
            "\n",
            " Epoch 422 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.158\n",
            "Validation Loss: 2.148\n",
            "\n",
            " Epoch 423 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.138\n",
            "Validation Loss: 2.146\n",
            "\n",
            " Epoch 424 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.116\n",
            "Validation Loss: 2.182\n",
            "\n",
            " Epoch 425 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.209\n",
            "\n",
            " Epoch 426 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.165\n",
            "\n",
            " Epoch 427 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.114\n",
            "Validation Loss: 2.178\n",
            "\n",
            " Epoch 428 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.122\n",
            "Validation Loss: 2.177\n",
            "\n",
            " Epoch 429 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.155\n",
            "Validation Loss: 2.186\n",
            "\n",
            " Epoch 430 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.128\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 431 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.171\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 432 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.227\n",
            "\n",
            " Epoch 433 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.182\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 434 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.131\n",
            "Validation Loss: 2.199\n",
            "\n",
            " Epoch 435 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.131\n",
            "Validation Loss: 2.155\n",
            "\n",
            " Epoch 436 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.233\n",
            "\n",
            " Epoch 437 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.124\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 438 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.169\n",
            "Validation Loss: 2.158\n",
            "\n",
            " Epoch 439 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.174\n",
            "\n",
            " Epoch 440 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.145\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 441 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.184\n",
            "\n",
            " Epoch 442 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.129\n",
            "Validation Loss: 2.162\n",
            "\n",
            " Epoch 443 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.151\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 444 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.137\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 445 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.188\n",
            "\n",
            " Epoch 446 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.144\n",
            "Validation Loss: 2.152\n",
            "\n",
            " Epoch 447 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.126\n",
            "Validation Loss: 2.218\n",
            "\n",
            " Epoch 448 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.148\n",
            "Validation Loss: 2.161\n",
            "\n",
            " Epoch 449 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.181\n",
            "Validation Loss: 2.160\n",
            "\n",
            " Epoch 450 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.149\n",
            "\n",
            " Epoch 451 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.171\n",
            "\n",
            " Epoch 452 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.132\n",
            "Validation Loss: 2.167\n",
            "\n",
            " Epoch 453 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.144\n",
            "Validation Loss: 2.218\n",
            "\n",
            " Epoch 454 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.108\n",
            "Validation Loss: 2.221\n",
            "\n",
            " Epoch 455 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.162\n",
            "Validation Loss: 2.206\n",
            "\n",
            " Epoch 456 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.140\n",
            "Validation Loss: 2.170\n",
            "\n",
            " Epoch 457 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.118\n",
            "Validation Loss: 2.181\n",
            "\n",
            " Epoch 458 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.159\n",
            "Validation Loss: 2.155\n",
            "\n",
            " Epoch 459 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.141\n",
            "Validation Loss: 2.158\n",
            "\n",
            " Epoch 460 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.133\n",
            "Validation Loss: 2.149\n",
            "\n",
            " Epoch 461 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.153\n",
            "Validation Loss: 2.142\n",
            "\n",
            " Epoch 462 / 500\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.114\n",
            "Validation Loss: 2.168\n",
            "\n",
            " Epoch 463 / 500\n",
            "\n",
            "Evaluating...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX1uTwjUPY6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}